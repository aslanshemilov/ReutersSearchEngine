{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import nltk\n",
    "import collections\n",
    "import sys\n",
    "from math import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If there are n terms in the query, we must iterate n times.\n",
    "#For every term in the query, find the document length, average document length, and the \n",
    "    #frequency of the term in the document. Using the formula, add them all as the ranking\n",
    "    #score and use it as a statistic for the document.\n",
    "    \n",
    "def calculateRSVd(queryList, finalResult):\n",
    "    match = []\n",
    "    RSVd = 0;\n",
    "    for docID in finalResult:\n",
    "        for word in queryList: \n",
    "            if word == 'and':\n",
    "                pass\n",
    "            else:\n",
    "                match = calculateQueryWordRef(word, docID)\n",
    "                docLength = match[0]\n",
    "                termFrequencyInDoc = match[1]\n",
    "                docFrequency = match[2]\n",
    "                del match[:]\n",
    "                step1 = (1 - b) + b * (docLength / avgDocLength)\n",
    "                step2 = (k + 1) * termFrequencyInDoc / (k * step1 + termFrequencyInDoc)\n",
    "                RSVdq = log(float(docCounter / docFrequency)) * step2\n",
    "                \n",
    "                RSVd = RSVd + RSVdq\n",
    "                docRanking[docID] = RSVd\n",
    "                \n",
    "    return docRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateQueryWordRef(key, docID):\n",
    "    #Initalization\n",
    "    match = []\n",
    "    dictAid = {}\n",
    "    fileName = \"final_dict.json\"\n",
    "    termFrequencyInDoc = 0\n",
    "    docFrequency = 0\n",
    "    #Convert key to lowercase\n",
    "    key = key.lower()\n",
    "    with open(filename, 'r') as json_file: \n",
    "        finalDict = json.load(json_file)\n",
    "        dictAid = finalDict.get(key)\n",
    "        if finalDict.has_key(key):\n",
    "            termFrequencyInDoc = dictAid.get(docID)\n",
    "            docFrequency = len(dictAid)\n",
    "            \n",
    "        match.append(docLength)\n",
    "        match.append(termFrequencyInDoc)\n",
    "        match.append(docFrequency)\n",
    "    json_file.close()\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open JSON file and do single query\n",
    "def findMatch(key): \n",
    "    match = []\n",
    "    fileName = \"final_dict.json\"\n",
    "    #Convert key to lowercase\n",
    "    key = key.lower()\n",
    "    with open(filename, 'r') as json_file: \n",
    "        json_dict = json.load(json_file)\n",
    "        if json_dict.has_key(key):\n",
    "            for docID in json_dict[key].keys():\n",
    "                match.append(docID)\n",
    "        else:\n",
    "            print \"There is no key named\", key\n",
    "    json_file.close()\n",
    "    match.sort()\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortMatch(list):\n",
    "    newList = []\n",
    "    for word in list: \n",
    "        word = int(word)\n",
    "        newList.append(word)\n",
    "    newList.sort()\n",
    "    for word in newList: \n",
    "        word = \"u'    \" + str(word) + \"   '\"\n",
    "    print 'list is', newList\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleWordQuery(queryList):\n",
    "    newMatch = {}\n",
    "    result_and = []\n",
    "    for key in queryList:\n",
    "        match = findMatch(key)\n",
    "        newMatch[key] = sorted(match[key])\n",
    "        print key, 'exists in : ', newMatch[key], 'docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopWords30(word):\n",
    "    words30= ['',' ','a', 'about', 'above', 'across', 'after', \n",
    "              'again', 'against','all',\n",
    "              'almost','alone','along','already','also',\n",
    "              'although','always','among','an','and',\n",
    "              'another','any','anybody','anyone','anything','anywhere','are',' ','areas',\n",
    "              'around','as','\\n']\n",
    "    if word in words30: \n",
    "        x = True\n",
    "        return x;\n",
    "    else: \n",
    "        x = False\n",
    "        return x\n",
    "    \n",
    "def stopWords150(word):\n",
    "    words150 = ['',' ','\\n','a', 'about', 'above', 'after', 'again', 'against', 'all', \n",
    "                'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'an', \n",
    "                'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'around', \n",
    "                'as', 'at', 'away', 'been', 'but', 'even', 'evenly', 'ever', 'every', 'everybody', \n",
    "                'everyone', 'everything', 'everywhere', 'for', 'had', 'has', 'have', 'he', 'her', \n",
    "                'here', 'herself', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', \n",
    "                'if', 'in', 'interests', 'interesting', 'into', 'is', 'it', 'its', 'itself', 'just', \n",
    "                'last', 'me', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', \n",
    "                'of', 'off', 'often', 'on', 'once', 'only', 'or', 'other', 'others', 'our', 'out', 'over', \n",
    "                'second', 'seconds', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', \n",
    "                'she', 'should', 'since', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', \n",
    "                'still', 'such', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', \n",
    "                'these', 'they', 'things', 'this', 'those', 'though', 'thoughts', 'through', 'thus', \n",
    "                'to', 'too', 'until', 'us', 'very', 'was', 'ways', 'we', 'were', 'what', 'when', 'where', \n",
    "                'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', \n",
    "                'would', 'yet', 'you', 'your', 'yours']\n",
    "    if word in words150:\n",
    "        x = True\n",
    "        return x;\n",
    "    else:\n",
    "        x = False\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeTokenDict(dict1, dict2):\n",
    "    for key in dict2.keys():\n",
    "        if key in dict1.keys():\n",
    "            for docID in dict2[key].keys():\n",
    "                if docid in dict1[key].keys():\n",
    "                    dict1[key][docid] += dict2[key][docid]\n",
    "                else:\n",
    "                    dict1[key][docid] = dict2[key][docid]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1\n",
    "\n",
    "            \n",
    "def ranking(queryList, finalResult):\n",
    "    if(len(finalResult) > 0):\n",
    "        docRanking = calculateRSVd(queryList, finalResult)\n",
    "        if len(docRanking) == 1:\n",
    "            print docRanking\n",
    "        else: \n",
    "            print sorted(docRanking.items(), lambda x, y: cmp(x[1], y[1]), reverse = True)\n",
    "        docRanking.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Start the program-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print '----------------------------Start the program-------------------------------'\n",
    "\n",
    "tokendict = {}\n",
    "docLenStore = {} #store the docLen of each doc\n",
    "docRanking = {} #store docid:rankingScore\n",
    "fileCounter =0\n",
    "path = 'data-files/reuters21578/'\n",
    "delimite = ' '\n",
    "sgmfiles = glob.glob(path + 'reut2-*.sgm')\n",
    "memorylimit = 1024 *3\n",
    "k = 1.2\n",
    "b = 0.75\n",
    "start = time.clock()\n",
    "docCounter = 0  # count the number of doc\n",
    "doclenCounterList = []; #store every doclen, in order to calculate the avgdoclen in convience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-19-6dddc89cccde>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-6dddc89cccde>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    stopwords += 1\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "print '---Open the files---'\n",
    "print '---Split the files---'\n",
    "fileName = 'file' + str(fileCounter) + '.json' #change to json file to store the dictionary\n",
    "target = open(fileName,'w')\n",
    "target.truncate()\n",
    "stopWords = 0\n",
    "numbers = 0\n",
    "word_flag = True;\n",
    "\n",
    "for i in range(len(sgmfiles)):\n",
    "    with open(sgmfiles[i],'r') as f:\n",
    "        totalDocLen = 0;\n",
    "        flag = False\n",
    "        docLength = 0\n",
    "        for line in f:\n",
    "            line = line.replace('<', ' <');\n",
    "            line = line.replace('>', '> ');\n",
    "            line = line.replace('\\n', ' ');\n",
    "            line = line.replace('\\t', ' ');\n",
    "            line = line.replace('.','');\n",
    "            line = line.replace(',',' ')\n",
    "            line = line.replace('.',' ')\n",
    "            line = line.replace('}',' ')\n",
    "            line = line.replace('{',' ')\n",
    "            line = line.replace('&',' ')\n",
    "            line = line.replace('-',' ')\n",
    "            line = line.replace('\"',' ')\n",
    "            line = line.replace(';',' ')\n",
    "            line = line.replace(\"(\",' ')\n",
    "            line = line.replace(\")\",' ')\n",
    "            line = line.replace('*','')\n",
    "            line = line.replace(':',' ')\n",
    "            line = line.replace('+',' ')\n",
    "            line = line.replace('$',' ')\n",
    "            line = line.replace(\"'\",'')\n",
    "            line = line.replace(\"=\",' ')\n",
    "            if('<REUTERS' in line):\n",
    "                docIDPreviousOne = 0 #the first docid must be 0,in order to make difference with later docid\n",
    "\n",
    "                docID = line[line.find('NEWID'): ]\n",
    "                docID = docID.replace('\"','')\n",
    "                docID = docID.replace('>','')\n",
    "                docID = docID.replace('NEWID','')\n",
    "                docID = docID.replace('=','')\n",
    "                \n",
    "                if(docid != docid_previousone):\n",
    "                    docCounter += 1 #calculate the total number of docs = (N)\n",
    "                    #store the totalDocLen in the docLenStore\n",
    "                docLenStore[docID] = totalDocLen\n",
    "                totalDocLen = 0\n",
    "                    #reset totalDocLen\n",
    "            for word in line.split(' '):\n",
    "                word = word.strip()\n",
    "                if(word.lower() == '<body>' or word.lower() == '<title>'):\n",
    "                    flag = True\n",
    "                    pass\n",
    "                elif(word.lower() == '</body>' or word.lower() == '</title>'):\n",
    "                    flag = False\n",
    "                    pass\n",
    "                elif flag:\n",
    "                    if(word==' ' or word == '' or '&#' in word or '/' in word or '[' in word or ']' in word or '@' in word or '=' in word or '?' in word or '#' in word or '&' in word ):\n",
    "                        pass\n",
    "                    elif(word.isdigit()): #remove numbers\n",
    "                        numbers += 1\n",
    "                        pass\n",
    "                    elif (stopWords150(word)): #remove 150 stopwords\n",
    "                        stopwords += 1\n",
    "                        pass\n",
    "                    else:\n",
    "                        word = word.replace('<','')\n",
    "                        word = word.replace('>','')\n",
    "                        word = word.lower()\n",
    "                        totalDocLen += 1\n",
    "                        if word in tokendict:\n",
    "                            if tokendict[word].has_key(docid):\n",
    "                                tokendict[word][docid] += 1\n",
    "                            else:\n",
    "                                tokendict[word][docid] = 1\n",
    "                        else:\n",
    "                            tokendict[word] = {}\n",
    "                            tokendict[word][docid] = 1\n",
    "                            \n",
    "                #Check memory size\n",
    "                if ((sys.getsizeof(tokendict)/1024) >= memorylimit):\n",
    "                    fileCounter = fileCounter + 1\n",
    "                    tokendict = collections.OrderedDict(sorted(tokendict.items()))\n",
    "                    filename = \"file\" + str(fileCounter) +\".json\"\n",
    "                    data_string = json.dumps(tokendict, encoding = 'latin-1')\n",
    "                    target = open(filename,'w')\n",
    "                    target.write(data_string)\n",
    "                    target.close()\n",
    "                    tokendict.clear()\n",
    "                    filename_doclenStore = \"doclenStore.json\"\n",
    "                    data_string1 = json.dumps(docLenStore)\n",
    "                    target1 = open(filename_doclenStore,'w')\n",
    "                    target1.write(data_string1)\n",
    "                    target1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
