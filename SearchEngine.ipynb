{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import nltk\n",
    "import collections\n",
    "import sys\n",
    "from math import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If there are n terms in the query, we must iterate n times.\n",
    "#For every term in the query, find the document length, average document length, and the \n",
    "    #frequency of the term in the document. Using the formula, add them all as the ranking\n",
    "    #score and use it as a statistic for the document.\n",
    "    \n",
    "def calculateRSVd(queryList, finalResult):\n",
    "    match = []\n",
    "    RSVd = 0;\n",
    "    for docID in finalResult:\n",
    "        for word in queryList: \n",
    "            if word == 'and':\n",
    "                pass\n",
    "            else:\n",
    "                match = calculateQueryWordRef(word, docID)\n",
    "                docLength = match[0]\n",
    "                termFrequencyInDoc = match[1]\n",
    "                docFrequency = match[2]\n",
    "                del match[:]\n",
    "                step1 = (1 - b) + b * (docLength / avgDocLength)\n",
    "                step2 = (k + 1) * termFrequencyInDoc / (k * step1 + termFrequencyInDoc)\n",
    "                RSVdq = log(float(docCounter / docFrequency)) * step2\n",
    "                \n",
    "                RSVd = RSVd + RSVdq\n",
    "                docRanking[docID] = RSVd\n",
    "                \n",
    "    return docRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateQueryWordRef(key, docID):\n",
    "    #Initalization\n",
    "    match = []\n",
    "    dictAid = {}\n",
    "    fileName = \"final_dict.json\"\n",
    "    termFrequencyInDoc = 0\n",
    "    docFrequency = 0\n",
    "    #Convert key to lowercase\n",
    "    key = key.lower()\n",
    "    with open(filename, 'r') as json_file: \n",
    "        finalDict = json.load(json_file)\n",
    "        dictAid = finalDict.get(key)\n",
    "        if finalDict.has_key(key):\n",
    "            termFrequencyInDoc = dictAid.get(docID)\n",
    "            docFrequency = len(dictAid)\n",
    "            \n",
    "        match.append(docLength)\n",
    "        match.append(termFrequencyInDoc)\n",
    "        match.append(docFrequency)\n",
    "    json_file.close()\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open JSON file and do single query\n",
    "def findMatch(key): \n",
    "    match = []\n",
    "    fileName = \"final_dict.json\"\n",
    "    #Convert key to lowercase\n",
    "    key = key.lower()\n",
    "    with open(filename, 'r') as json_file: \n",
    "        json_dict = json.load(json_file)\n",
    "        if json_dict.has_key(key):\n",
    "            for docID in json_dict[key].keys():\n",
    "                match.append(docID)\n",
    "        else:\n",
    "            print \"There is no key named\", key\n",
    "    json_file.close()\n",
    "    match.sort()\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortMatch(list):\n",
    "    newList = []\n",
    "    for word in list: \n",
    "        word = int(word)\n",
    "        newList.append(word)\n",
    "    newList.sort()\n",
    "    for word in newList: \n",
    "        word = \"u'    \" + str(word) + \"   '\"\n",
    "    print 'list is', newList\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleWordQuery(queryList):\n",
    "    newMatch = {}\n",
    "    result_and = []\n",
    "    for key in queryList:\n",
    "        match = findMatch(key)\n",
    "        newMatch[key] = sorted(match[key])\n",
    "        print key, 'exists in : ', newMatch[key], 'docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopWords30(word):\n",
    "    words30= ['',' ','a', 'about', 'above', 'across', 'after', \n",
    "              'again', 'against','all',\n",
    "              'almost','alone','along','already','also',\n",
    "              'although','always','among','an','and',\n",
    "              'another','any','anybody','anyone','anything','anywhere','are',' ','areas',\n",
    "              'around','as','\\n']\n",
    "    if word in words30: \n",
    "        x = True\n",
    "        return x;\n",
    "    else: \n",
    "        x = False\n",
    "        return x\n",
    "    \n",
    "def stopWords150(word):\n",
    "    words150 = ['',' ','\\n','a', 'about', 'above', 'after', 'again', 'against', 'all', \n",
    "                'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'an', \n",
    "                'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'around', \n",
    "                'as', 'at', 'away', 'been', 'but', 'even', 'evenly', 'ever', 'every', 'everybody', \n",
    "                'everyone', 'everything', 'everywhere', 'for', 'had', 'has', 'have', 'he', 'her', \n",
    "                'here', 'herself', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', \n",
    "                'if', 'in', 'interests', 'interesting', 'into', 'is', 'it', 'its', 'itself', 'just', \n",
    "                'last', 'me', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', \n",
    "                'of', 'off', 'often', 'on', 'once', 'only', 'or', 'other', 'others', 'our', 'out', 'over', \n",
    "                'second', 'seconds', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', \n",
    "                'she', 'should', 'since', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', \n",
    "                'still', 'such', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', \n",
    "                'these', 'they', 'things', 'this', 'those', 'though', 'thoughts', 'through', 'thus', \n",
    "                'to', 'too', 'until', 'us', 'very', 'was', 'ways', 'we', 'were', 'what', 'when', 'where', \n",
    "                'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', \n",
    "                'would', 'yet', 'you', 'your', 'yours']\n",
    "    if word in words150:\n",
    "        x = True\n",
    "        return x;\n",
    "    else:\n",
    "        x = False\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeTokenDict(dict1, dict2):\n",
    "    for key in dict2.keys():\n",
    "        if key in dict1.keys():\n",
    "            for docID in dict2[key].keys():\n",
    "                if docid in dict1[key].keys():\n",
    "                    dict1[key][docid] += dict2[key][docid]\n",
    "                else:\n",
    "                    dict1[key][docid] = dict2[key][docid]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1\n",
    "\n",
    "            \n",
    "def ranking(queryList, finalResult):\n",
    "    if(len(finalResult) > 0):\n",
    "        docRanking = calculateRSVd(queryList, finalResult)\n",
    "        if len(docRanking) == 1:\n",
    "            print docRanking\n",
    "        else: \n",
    "            print sorted(docRanking.items(), lambda x, y: cmp(x[1], y[1]), reverse = True)\n",
    "        docRanking.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Start the program-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print '----------------------------Start the program-------------------------------'\n",
    "\n",
    "tokendict = {}\n",
    "docLenStore = {} #store the docLen of each doc\n",
    "docRanking = {} #store docid:rankingScore\n",
    "fileCounter =0\n",
    "path = 'data-files/reuters21578/'\n",
    "delimite = ' '\n",
    "sgmfiles = glob.glob(path + 'reut2-*.sgm')\n",
    "memorylimit = 1024 *3\n",
    "k = 1.2\n",
    "b = 0.75\n",
    "start = time.clock()\n",
    "docCounter = 0  # count the number of doc\n",
    "doclenCounterList = []; #store every doclen, in order to calculate the avgdoclen in convience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Open the files---\n",
      "---Split the files---\n"
     ]
    }
   ],
   "source": [
    "print '---Open the files---'\n",
    "print '---Split the files---'\n",
    "fileName = 'file' + str(fileCounter) + '.json' #change to json file to store the dictionary\n",
    "target = open(fileName,'w')\n",
    "target.truncate()\n",
    "stopWords = 0\n",
    "numbers = 0\n",
    "word_flag = True;\n",
    "\n",
    "for i in range(len(sgmfiles)):\n",
    "    with open(sgmfiles[i],'r') as f:\n",
    "        totalDocLen = 0;\n",
    "        flag = False\n",
    "        docLength = 0\n",
    "        for line in f:\n",
    "            line = line.replace('<', ' <');\n",
    "            line = line.replace('>', '> ');\n",
    "            line = line.replace('\\n', ' ');\n",
    "            line = line.replace('\\t', ' ');\n",
    "            line = line.replace('.','');\n",
    "            line = line.replace(',',' ')\n",
    "            line = line.replace('.',' ')\n",
    "            line = line.replace('}',' ')\n",
    "            line = line.replace('{',' ')\n",
    "            line = line.replace('&',' ')\n",
    "            line = line.replace('-',' ')\n",
    "            line = line.replace('\"',' ')\n",
    "            line = line.replace(';',' ')\n",
    "            line = line.replace(\"(\",' ')\n",
    "            line = line.replace(\")\",' ')\n",
    "            line = line.replace('*','')\n",
    "            line = line.replace(':',' ')\n",
    "            line = line.replace('+',' ')\n",
    "            line = line.replace('$',' ')\n",
    "            line = line.replace(\"'\",'')\n",
    "            line = line.replace(\"=\",' ')\n",
    "            if('<REUTERS' in line):\n",
    "                docIDPreviousOne = 0 #the first docid must be 0,in order to make difference with later docid\n",
    "\n",
    "                docID = line[line.find('NEWID'): ]\n",
    "                docID = docID.replace('\"','')\n",
    "                docID = docID.replace('>','')\n",
    "                docID = docID.replace('NEWID','')\n",
    "                docID = docID.replace('=','')\n",
    "                \n",
    "                if(docid != docid_previousone):\n",
    "                    docCounter += 1 #calculate the total number of docs = (N)\n",
    "                    #store the totalDocLen in the docLenStore\n",
    "                docLenStore[docID] = totalDocLen\n",
    "                totalDocLen = 0\n",
    "                    #reset totalDocLen\n",
    "            for word in line.split(' '):\n",
    "                word = word.strip()\n",
    "                if(word.lower() == '<body>' or word.lower() == '<title>'):\n",
    "                    flag = True\n",
    "                    pass\n",
    "                elif(word.lower() == '</body>' or word.lower() == '</title>'):\n",
    "                    flag = False\n",
    "                    pass\n",
    "                elif flag:\n",
    "                    if(word==' ' or word == '' or '&#' in word or '/' in word or '[' in word or ']' in word or '@' in word or '=' in word or '?' in word or '#' in word or '&' in word ):\n",
    "                        pass\n",
    "                    elif(word.isdigit()): #remove numbers\n",
    "                        numbers += 1\n",
    "                        pass\n",
    "                    elif (stopWords150(word)): #remove 150 stopwords\n",
    "                        stopWords += 1\n",
    "                        pass\n",
    "                    else:\n",
    "                        word = word.replace('<','')\n",
    "                        word = word.replace('>','')\n",
    "                        word = word.lower()\n",
    "                        totalDocLen += 1\n",
    "                        if word in tokendict:\n",
    "                            if tokendict[word].has_key(docid):\n",
    "                                tokendict[word][docid] += 1\n",
    "                            else:\n",
    "                                tokendict[word][docid] = 1\n",
    "                        else:\n",
    "                            tokendict[word] = {}\n",
    "                            tokendict[word][docid] = 1\n",
    "                            \n",
    "                #Check memory size\n",
    "                if ((sys.getsizeof(tokendict)/1024) >= memorylimit):\n",
    "                    fileCounter = fileCounter + 1\n",
    "                    tokendict = collections.OrderedDict(sorted(tokendict.items()))\n",
    "                    fileName = \"file\" + str(fileCounter) +\".json\"\n",
    "                    data_string = json.dumps(tokendict, encoding = 'latin-1')\n",
    "                    target = open(fileName,'w')\n",
    "                    target.write(data_string)\n",
    "                    target.close()\n",
    "                    tokendict.clear()\n",
    "                    filename_doclenStore = \"doclenStore.json\"\n",
    "                    data_string1 = json.dumps(docLenStore)\n",
    "                    target1 = open(filename_doclenStore,'w')\n",
    "                    target1.write(data_string1)\n",
    "                    target1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of numbers deleted =  0\n",
      "# of stopwords deleted = "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-17a436f765aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'# of numbers deleted = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m'# of stopwords deleted = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m#---------------------Quary part------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmatch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#if the block is not fulled in final,we need to get this part too\n",
    "tokendict = collections.OrderedDict(sorted(tokendict.items()))\n",
    "\n",
    "#Calculate average document length\n",
    "totalLen = 0\n",
    "for docid, docLen in docLenStore.items():\n",
    "    totalLen += docLen\n",
    "    avgdoclen = totalLen/len(docLenStore)\n",
    "    print avgdoclen\n",
    "\n",
    "#data_string = unicode(json.dumps(tokendict),errors = 'ignore')\n",
    "data_string = json.dumps(tokendict,encoding = 'latin-1')\n",
    "target = open(fileName,'w')\n",
    "target.write(data_string)\n",
    "target.close()\n",
    "filename_doclenStore = \"doclenStore.json\"\n",
    "data_string1 = json.dumps(docLenStore)\n",
    "target1 = open(filename_doclenStore,'w')\n",
    "target1.write(data_string1)\n",
    "target1.close()\n",
    "#---------------------merge blocks----------------------------------------------------------------------\n",
    "#pay attention that when merge them ,need to check the term part\n",
    "mergeCounter = 0\n",
    "final_dict={}\n",
    "#print '-------------file counter: ',fileCounter~~~\n",
    "#merge each small dictionary(merge those json file)\n",
    "for mergeCounter in range(fileCounter + 1):\n",
    "    fileName = \"file\"+str(mergeCounter)+\".json\"\n",
    "    with open(fileName,'r') as dict2:\n",
    "        dict2 = json.load(dict2)\n",
    "        final_dict = mergeTokenDict(final_dict,dict2)\n",
    "        \n",
    "#merge this dictionary docLenStore-->ignore\n",
    "fileName = \"final_dict.json\"\n",
    "data_string = json.dumps(final_dict)\n",
    "target = open(fileName, 'w')\n",
    "target.write(data_string)\n",
    "target.close()\n",
    "\n",
    "print '# of numbers deleted = ', numbers\n",
    "print '# of stopWords deleted = ', stopWords\n",
    "#---------------------Quary part------------------------------\n",
    "match1 = {}\n",
    "query_resluts = {}\n",
    "\n",
    "final_result=[]# an empty smallList, in order to do the insection operation,and store the final_result in final\n",
    "andFlag = False\n",
    "orFlag = False\n",
    "queryFlag = True\n",
    "query_list=[] # store the query words\n",
    "\n",
    "while queryFlag:\n",
    "\tquery = raw_input(\"--------Please enter your Query-----------\\n\\n\")\n",
    "\tsingleResultList = []\n",
    "\tquery_list = query.split(' ')\n",
    "\tprint query_list\n",
    "\t#single word query\n",
    "\tif len(query_list) == 1:\n",
    "\t\t#query_resluts = findMatch(query)\n",
    "\t\tfinal_result = findMatch(query)\n",
    "\t\tprint 'The result of',query,'is',final_result#query_resluts\n",
    "\t\tRanking(query_list,final_result)\n",
    "\t\t# if(len(final_result) > 0):\n",
    "\t\t# \tdoc_ranking = calculate_RSVd(query_list,final_result)\n",
    "\t\t# \tif len(doc_ranking) == 1:\n",
    "\t\t# \t\tprint doc_ranking\n",
    "\t\t# \telse:\n",
    "\t\t# \t#sort based on ranking\n",
    "\t\t# \t\tprint sorted(doc_ranking.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)# decrese ordering\n",
    "\t\t# \tdoc_ranking.clear()\n",
    "\n",
    "\t#multiple word query\n",
    "\telse:#and operation (multiple and)\n",
    "\t\t#if 'and' in query_list:\n",
    "\t\tfor word in query_list:\n",
    "\t\t\t\t#do single query for each word and get the interaction of their results\n",
    "\t\t\tif (word == 'and' or word == 'or'):\n",
    "\t\t\t\t\t#query_list.remove('and')\n",
    "\t\t\t\tpass\n",
    "\t\t\telse: #do single query for each word\n",
    "\t\t\t\tlist_of_matchedItems = findMatch(word)\n",
    "\t\t\t\tsingleResultList.append(list_of_matchedItems)\n",
    "\t\t\t\tprint('word is:',word,findMatch(word))   #('word is: ', {'textile': [354, 429, 867, 891, 956, 1002, 1197, 1226, 1509, 1594, 1809, 1955]})\n",
    "\t\t\t\t\t#have multiple singleQuery results,\n",
    "\t\tprint 'Consumed time =',time.clock()-start,'ms'\n",
    "\t\t\t#do interaction\n",
    "\t\tp = 0\n",
    "\t\tfinal_result = singleResultList[0]\n",
    "\t\tfor index in xrange(1, len(singleResultList)):\n",
    "\n",
    "\t\t\tfinal_result = list(set(final_result) & set(singleResultList[index]))\n",
    "\t\tprint 'The final result is:',final_result\n",
    "\t\tRanking(query_list,final_result)\n",
    "\n",
    "#---------------------------for the Ranking part------------------------------\n",
    "#just use Ranking function() to simplify!!!\n",
    "#for each doc in final_result, do the RSVd ranking, and bound rankingscore with docid\n",
    "\t\t# if(len(final_result) > 0):\n",
    "\t\t# \tdoc_ranking = calculate_RSVd(query_list,final_result)\n",
    "\t\t# \tif len(doc_ranking) == 1:\n",
    "\t\t# \t\tprint doc_ranking\n",
    "\t\t# \telse:\n",
    "\t\t# \t#sort based on ranking\n",
    "\t\t# \t\tprint sorted(doc_ranking.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)# decrese ordering\n",
    "\t\t# \tdoc_ranking.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenize--->spimi---->(inverted index)//check size,sort,wirte into q block file\n",
    "\n",
    "#remove stop word,ask alaa (check)\n",
    "\n",
    "#open block files,merge them into a big dictionary(consider the data structure(use dictionary{,dictinary{, tuple}} & which part need to be recorded)\n",
    "#using the dictionary,realize the query(single word, and,or),print the result out\n",
    "\n",
    "#and operation,do the insection operation or two lists\n",
    "#or operation, just add them together\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#calculate the formula,print the result out\n",
    "\n",
    "\n",
    "#lengthCouter if docid changes, record the len of the doc,and record it into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
